[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.17905s.

Running...
 Run 1:
  0.00458s: started 0
  4.82331s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00558s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.83 seconds.
Writing data & meta-data took 0.01167s.
Overall took 5.02012s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04677s.

Running...
 Run 1:
  0.00115s: started 0
  4.56574s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00077s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.57 seconds.
Writing data & meta-data took 0.00896s.
Overall took 4.62253s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02944s.

Running...
 Run 1:
  0.0011s: started 0
  4.57799s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00078s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.58 seconds.
Writing data & meta-data took 0.01331s.
Overall took 4.62181s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04249s.

Running...
 Run 1:
  0.0011s: started 0
  4.57759s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00077s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.58 seconds.
Writing data & meta-data took 0.00878s.
Overall took 4.62991s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0497s.

Running...
 Run 1:
  0.00112s: started 0
  4.56155s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00083s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.56 seconds.
Writing data & meta-data took 0.00933s.
Overall took 4.6217s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04121s.

Running...
 Run 1:
  0.00112s: started 0
  4.21848s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.22 seconds.
Writing data & meta-data took 0.00868s.
Overall took 4.26947s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02798s.

Running...
 Run 1:
  0.00113s: started 0
  4.34844s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.35 seconds.
Writing data & meta-data took 0.00853s.
Overall took 4.38607s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03193s.

Running...
 Run 1:
  0.00125s: started 0
  4.23152s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.23 seconds.
Writing data & meta-data took 0.0079s.
Overall took 4.27245s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02773s.

Running...
 Run 1:
  0.00111s: started 0
  4.66295s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.66 seconds.
Writing data & meta-data took 0.00838s.
Overall took 4.70017s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04465s.

Running...
 Run 1:
  0.00112s: started 0
  5.16626s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 5.17 seconds.
Writing data & meta-data took 0.00862s.
Overall took 5.22062s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02977s.

Running...
 Run 1:
  0.00109s: started 0
  9.03989s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 9.04 seconds.
Writing data & meta-data took 0.00741s.
Overall took 9.07817s.
+ for schedule in ./apps_mix/gpubench/pp-baseline/1B ./apps_mix/gpubench/pp-baseline/4B ./apps_mix/gpubench/pp-baseline/8B ./apps_mix/gpubench/pp-baseline/64B ./apps_mix/gpubench/pp-baseline/512B ./apps_mix/gpubench/pp-baseline/4KiB ./apps_mix/gpubench/pp-baseline/32KiB ./apps_mix/gpubench/pp-baseline/256KiB ./apps_mix/gpubench/pp-baseline/2MiB ./apps_mix/gpubench/pp-baseline/16MiB ./apps_mix/gpubench/pp-baseline/128MiB ./apps_mix/gpubench/pp-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-baseline/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-baseline/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04119s.

Running...
 Run 1:
  0.00109s: started 0
  39.48652s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00087s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.49 seconds.
Writing data & meta-data took 0.00884s.
Overall took 39.53775s.
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05836s.

Running...
 Run 1:
  0.00111s: started 0
  6.27495s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00082s.
 Run 2:
  0.09806s: started 0
  5.50358s: awaited 0
  Data collection took 0.00011s.
  Convergence check took 0.00072s.
Completed after reaching confidence interval, terminated after 2 runs taking 11.78 seconds.
Writing data & meta-data took 0.00919s.
Overall took 11.84833s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04199s.

Running...
 Run 1:
  0.00131s: started 0
  6.15896s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00082s.
 Run 2:
  0.05169s: started 0
  5.58956s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00075s.
 Run 3:
  0.0812s: started 0
  5.26967s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
Completed after reaching confidence interval, terminated after 3 runs taking 17.02 seconds.
Writing data & meta-data took 0.00917s.
Overall took 17.07235s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02876s.

Running...
 Run 1:
  0.0011s: started 0
  6.73669s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00085s.
 Run 2:
  0.13909s: started 0
  5.66686s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
Completed after reaching confidence interval, terminated after 2 runs taking 12.41 seconds.
Writing data & meta-data took 0.00944s.
Overall took 12.44384s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04062s.

Running...
 Run 1:
  0.00116s: started 0
  6.64847s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00083s.
 Run 2:
  0.00129s: started 0
  6.00957s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
 Run 3:
  0.0516s: started 0
  5.64734s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00071s.
Completed after reaching confidence interval, terminated after 3 runs taking 18.31 seconds.
Writing data & meta-data took 0.00874s.
Overall took 18.35771s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04458s.

Running...
 Run 1:
  0.00109s: started 0
  6.38499s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.0008s.
 Run 2:
  0.05313s: started 0
  6.00889s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
Completed after reaching confidence interval, terminated after 2 runs taking 12.4 seconds.
Writing data & meta-data took 0.00898s.
Overall took 12.44948s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04995s.

Running...
 Run 1:
  0.00109s: started 0
  6.33704s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
 Run 2:
  0.08315s: started 0
  5.74422s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00071s.
Completed after reaching confidence interval, terminated after 2 runs taking 12.08 seconds.
Writing data & meta-data took 0.00964s.
Overall took 12.14286s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04101s.

Running...
 Run 1:
  0.00118s: started 0
  6.27456s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00084s.
 Run 2:
  0.15336s: started 0
  5.61683s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00071s.
Completed after reaching confidence interval, terminated after 2 runs taking 11.89 seconds.
Writing data & meta-data took 0.00922s.
Overall took 11.94366s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04627s.

Running...
 Run 1:
  0.0011s: started 0
  6.34612s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00089s.
 Run 2:
  0.15024s: started 0
  5.79285s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
Completed after reaching confidence interval, terminated after 2 runs taking 12.14 seconds.
Writing data & meta-data took 0.00879s.
Overall took 12.19616s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03867s.

Running...
 Run 1:
  0.00114s: started 0
  6.15465s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.16 seconds.
Writing data & meta-data took 0.00869s.
Overall took 6.20315s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04847s.

Running...
 Run 1:
  0.00113s: started 0
  8.30382s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 8.3 seconds.
Writing data & meta-data took 0.00828s.
Overall took 8.36172s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04513s.

Running...
 Run 1:
  0.00112s: started 0
  18.81504s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00089s.
Completed after reaching confidence interval, terminated after 1 runs taking 18.82 seconds.
Writing data & meta-data took 0.00809s.
Overall took 18.86944s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-baseline/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-baseline/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0472s.

Running...
 Run 1:
  0.00114s: started 0
  10.12252s: awaited 0
  10.12254s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.33
slurmstepd: error: *** STEP 45848.33 ON nid005128 CANCELLED AT 2024-03-22T11:10:19 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.33

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 2:
  0.00132s: started 0
  9.61395s: awaited 0
  9.61397s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.34
slurmstepd: error: *** STEP 45848.34 ON nid005128 CANCELLED AT 2024-03-22T11:10:28 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.34

  Data collection took 1e-05s.
  Convergence check took 1e-05s.
 Run 3:
  0.00134s: started 0
  9.09529s: awaited 0
  9.09531s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.35
slurmstepd: error: *** STEP 45848.35 ON nid005128 CANCELLED AT 2024-03-22T11:10:37 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.35

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 4:
  0.00129s: started 0
  10.15632s: awaited 0
  10.15634s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.36
slurmstepd: error: *** STEP 45848.36 ON nid005128 CANCELLED AT 2024-03-22T11:10:48 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.36

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 5:
  0.00131s: started 0
  9.59952s: awaited 0
  9.59954s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.37
slurmstepd: error: *** STEP 45848.37 ON nid005128 CANCELLED AT 2024-03-22T11:10:57 ***
srun: error: nid005128: tasks 0-1: Terminated
srun: Force Terminated StepId=45848.37

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 6:
  0.00135s: started 0
  9.15886s: awaited 0
  9.15889s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.38
slurmstepd: error: *** STEP 45848.38 ON nid005128 CANCELLED AT 2024-03-22T11:11:06 ***
srun: error: nid005128: tasks 0-1: Terminated
srun: Force Terminated StepId=45848.38

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 7:
  0.00132s: started 0
  9.4329s: awaited 0
  9.43292s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.39
slurmstepd: error: *** STEP 45848.39 ON nid005128 CANCELLED AT 2024-03-22T11:11:16 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.39

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 8:
  0.00128s: started 0
  9.9659s: awaited 0
  9.96592s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.40
slurmstepd: error: *** STEP 45848.40 ON nid005128 CANCELLED AT 2024-03-22T11:11:25 ***
srun: error: nid005128: tasks 0-1: Terminated
srun: Force Terminated StepId=45848.40

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 9:
  0.0013s: started 0
  9.69958s: awaited 0
  9.6996s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.41
slurmstepd: error: *** STEP 45848.41 ON nid005128 CANCELLED AT 2024-03-22T11:11:35 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.41

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 10:
  0.00129s: started 0
  9.7168s: awaited 0
  9.71682s: encountered an exception in 0
srun: error: nid005128: tasks 2-3: Segmentation fault
srun: Terminating StepId=45848.42
slurmstepd: error: *** STEP 45848.42 ON nid005128 CANCELLED AT 2024-03-22T11:11:45 ***
srun: error: nid005128: task 0: Terminated
srun: error: nid005128: task 1: Terminated
srun: Force Terminated StepId=45848.42

  Data collection took 1e-05s.
  Convergence check took 0.0s.
Completed maximum number of runs, terminated after 10 runs taking 96.57 seconds.
Traceback (most recent call last):
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 595, in <module>
    main()
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 580, in main
    log_data(out_format, data_directory+'/data', data_container_list)
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 184, in log_data
    dataframe.columns = columns
    ^^^^^^^^^^^^^^^^^
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/generic.py", line 6310, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/generic.py", line 813, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/internals/managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/internals/base.py", line 98, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 0 elements, new values have 2 elements
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.08159s.

Running...
 Run 1:
  0.00114s: started 0
  6.13322s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00078s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.13 seconds.
Writing data & meta-data took 0.0085s.
Overall took 6.22463s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04487s.

Running...
 Run 1:
  0.00112s: started 0
  6.31567s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.32 seconds.
Writing data & meta-data took 0.00779s.
Overall took 6.36943s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0451s.

Running...
 Run 1:
  0.0011s: started 0
  6.22574s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.23 seconds.
Writing data & meta-data took 0.0085s.
Overall took 6.28045s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04858s.

Running...
 Run 1:
  0.00112s: started 0
  6.26961s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.27 seconds.
Writing data & meta-data took 0.00857s.
Overall took 6.32786s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04932s.

Running...
 Run 1:
  0.00109s: started 0
  6.3301s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00085s.
 Run 2:
  0.10094s: started 0
  6.04095s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.0007s.
Completed after reaching confidence interval, terminated after 2 runs taking 12.37 seconds.
Writing data & meta-data took 0.00964s.
Overall took 12.43205s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03001s.

Running...
 Run 1:
  0.00113s: started 0
  5.99506s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00084s.
 Run 2:
  0.11191s: started 0
  5.75931s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00073s.
Completed after reaching confidence interval, terminated after 2 runs taking 11.76 seconds.
Writing data & meta-data took 0.00961s.
Overall took 11.79602s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03939s.

Running...
 Run 1:
  0.00111s: started 0
  6.39243s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00083s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.39 seconds.
Writing data & meta-data took 0.00928s.
Overall took 6.44223s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03493s.

Running...
 Run 1:
  0.00112s: started 0
  6.56087s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.56 seconds.
Writing data & meta-data took 0.00861s.
Overall took 6.60554s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0389s.

Running...
 Run 1:
  0.00123s: started 0
  6.87954s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.88 seconds.
Writing data & meta-data took 0.0084s.
Overall took 6.92796s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04279s.

Running...
 Run 1:
  0.00112s: started 0
  7.72251s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 7.72 seconds.
Writing data & meta-data took 0.00883s.
Overall took 7.77528s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03471s.

Running...
 Run 1:
  0.0011s: started 0
  17.62536s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00087s.
Completed after reaching confidence interval, terminated after 1 runs taking 17.63 seconds.
Writing data & meta-data took 0.00899s.
Overall took 17.67022s.
+ for schedule in ./apps_mix/gpubench/a2a-baseline/1B ./apps_mix/gpubench/a2a-baseline/4B ./apps_mix/gpubench/a2a-baseline/8B ./apps_mix/gpubench/a2a-baseline/64B ./apps_mix/gpubench/a2a-baseline/512B ./apps_mix/gpubench/a2a-baseline/4KiB ./apps_mix/gpubench/a2a-baseline/32KiB ./apps_mix/gpubench/a2a-baseline/256KiB ./apps_mix/gpubench/a2a-baseline/2MiB ./apps_mix/gpubench/a2a-baseline/16MiB ./apps_mix/gpubench/a2a-baseline/128MiB ./apps_mix/gpubench/a2a-baseline/1GiB ./apps_mix/gpubench/ar-baseline/1B ./apps_mix/gpubench/ar-baseline/4B ./apps_mix/gpubench/ar-baseline/8B ./apps_mix/gpubench/ar-baseline/64B ./apps_mix/gpubench/ar-baseline/512B ./apps_mix/gpubench/ar-baseline/4KiB ./apps_mix/gpubench/ar-baseline/32KiB ./apps_mix/gpubench/ar-baseline/256KiB ./apps_mix/gpubench/ar-baseline/2MiB ./apps_mix/gpubench/ar-baseline/16MiB ./apps_mix/gpubench/ar-baseline/128MiB ./apps_mix/gpubench/ar-baseline/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-baseline/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-baseline/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04889s.

Running...
 Run 1:
  0.00115s: started 0
  95.19639s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 95.2 seconds.
Writing data & meta-data took 0.00953s.
Overall took 95.25599s.
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05327s.

Running...
 Run 1:
  0.00116s: started 0
  10.11541s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00082s.
 Run 2:
  0.11516s: started 0
  9.90071s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00071s.
 Run 3:
  0.12356s: started 0
  10.12026s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00073s.
 Run 4:
  0.06241s: started 0
  9.89878s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00072s.
 Run 5:
  0.07277s: started 0
  9.8296s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00067s.
Completed after reaching confidence interval, terminated after 5 runs taking 49.87 seconds.
Writing data & meta-data took 0.01s.
Overall took 49.93294s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0526s.

Running...
 Run 1:
  0.0011s: started 0
  9.95802s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 9.96 seconds.
Writing data & meta-data took 0.00883s.
Overall took 10.02053s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02297s.

Running...
 Run 1:
  0.00108s: started 0
  10.07214s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00077s.
 Run 2:
  0.14728s: started 0
  10.28379s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00069s.
 Run 3:
  0.05262s: started 0
  9.97507s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00069s.
 Run 4:
  0.07671s: started 0
  9.58466s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00069s.
Completed after reaching confidence interval, terminated after 4 runs taking 39.92 seconds.
Writing data & meta-data took 0.00903s.
Overall took 39.95133s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04459s.

Running...
 Run 1:
  0.00113s: started 0
  10.1397s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00079s.
 Run 2:
  0.05621s: started 0
  9.6606s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.0007s.
 Run 3:
  0.06473s: started 0
  9.38553s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.0007s.
 Run 4:
  0.11832s: started 0
  9.86s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00069s.
 Run 5:
  0.12747s: started 0
  9.61631s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00071s.
Completed after reaching confidence interval, terminated after 5 runs taking 48.67 seconds.
Writing data & meta-data took 0.0089s.
Overall took 48.72022s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03261s.

Running...
 Run 1:
  0.00111s: started 0
  9.94823s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00079s.
 Run 2:
  0.11353s: started 0
  9.84877s: awaited 0
  Data collection took 0.00011s.
  Convergence check took 0.00065s.
Completed after reaching confidence interval, terminated after 2 runs taking 19.8 seconds.
Writing data & meta-data took 0.00888s.
Overall took 19.84037s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02618s.

Running...
 Run 1:
  0.00111s: started 0
  9.95496s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00077s.
Completed after reaching confidence interval, terminated after 1 runs taking 9.96 seconds.
Writing data & meta-data took 0.0092s.
Overall took 9.9914s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0382s.

Running...
 Run 1:
  0.00118s: started 0
  10.07683s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 10.08 seconds.
Writing data & meta-data took 0.00936s.
Overall took 10.12547s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04939s.

Running...
 Run 1:
  0.0012s: started 0
  9.93406s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 9.94 seconds.
Writing data & meta-data took 0.00839s.
Overall took 9.99293s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02226s.

Running...
 Run 1:
  0.00112s: started 0
  10.20413s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 10.21 seconds.
Writing data & meta-data took 0.00951s.
Overall took 10.23701s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04167s.

Running...
 Run 1:
  0.00112s: started 0
  10.27197s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 10.27 seconds.
Writing data & meta-data took 0.00856s.
Overall took 10.32329s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04728s.

Running...
 Run 1:
  0.00111s: started 0
  10.38697s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 10.39 seconds.
Writing data & meta-data took 0.00795s.
Overall took 10.44328s.
+ for schedule in ./apps_mix/gpubench/pp-nccl/1B ./apps_mix/gpubench/pp-nccl/4B ./apps_mix/gpubench/pp-nccl/8B ./apps_mix/gpubench/pp-nccl/64B ./apps_mix/gpubench/pp-nccl/512B ./apps_mix/gpubench/pp-nccl/4KiB ./apps_mix/gpubench/pp-nccl/32KiB ./apps_mix/gpubench/pp-nccl/256KiB ./apps_mix/gpubench/pp-nccl/2MiB ./apps_mix/gpubench/pp-nccl/16MiB ./apps_mix/gpubench/pp-nccl/128MiB ./apps_mix/gpubench/pp-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-nccl/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-nccl/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04841s.

Running...
 Run 1:
  0.0011s: started 0
  11.91657s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00078s.
Completed after reaching confidence interval, terminated after 1 runs taking 11.92 seconds.
Writing data & meta-data took 0.00847s.
Overall took 11.97455s.
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04815s.

Running...
 Run 1:
  0.00111s: started 0
  39.70033s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00086s.
 Run 2:
  0.00129s: started 0
  38.84359s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00076s.
 Run 3:
  0.11201s: started 0
  39.36883s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
 Run 4:
  0.06223s: started 0
  38.93891s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00076s.
 Run 5:
  0.11227s: started 0
  39.14974s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 6:
  0.11167s: started 0
  39.24956s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00077s.
 Run 7:
  0.12104s: started 0
  39.18879s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 8:
  0.04135s: started 0
  39.20852s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.0008s.
 Run 9:
  0.08174s: started 0
  39.14817s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00076s.
 Run 10:
  0.11271s: started 0
  39.48838s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00075s.
Completed maximum number of runs, terminated after 10 runs taking 392.3 seconds.
Writing data & meta-data took 0.03327s.
Overall took 392.37655s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04017s.

Running...
 Run 1:
  0.00114s: started 0
  39.79248s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00086s.
 Run 2:
  0.13273s: started 0
  39.18824s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00076s.
 Run 3:
  0.06359s: started 0
  39.22251s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 4:
  0.00144s: started 0
  38.92092s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00077s.
 Run 5:
  0.16815s: started 0
  39.41549s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00078s.
 Run 6:
  0.09177s: started 0
  39.63528s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00079s.
 Run 7:
  0.07544s: started 0
  39.28536s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00077s.
 Run 8:
  0.07911s: started 0
  39.12823s: awaited 0
  Data collection took 0.00015s.
  Convergence check took 0.00079s.
 Run 9:
  0.00127s: started 0
  39.39974s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00075s.
 Run 10:
  0.11917s: started 0
  38.85524s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00075s.
Completed maximum number of runs, terminated after 10 runs taking 392.85 seconds.
Writing data & meta-data took 0.01025s.
Overall took 392.90379s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03231s.

Running...
 Run 1:
  0.00111s: started 0
  39.66376s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00085s.
 Run 2:
  0.10268s: started 0
  39.0162s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 3:
  0.10554s: started 0
  38.91419s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00078s.
 Run 4:
  0.1504s: started 0
  39.16399s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00078s.
 Run 5:
  0.1154s: started 0
  39.09143s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 6:
  0.10306s: started 0
  39.03587s: awaited 0
  Data collection took 0.00015s.
  Convergence check took 0.00076s.
 Run 7:
  0.07619s: started 0
  39.21195s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00076s.
 Run 8:
  0.09324s: started 0
  39.37928s: awaited 0
  Data collection took 0.00015s.
  Convergence check took 0.00079s.
 Run 9:
  0.10292s: started 0
  39.38915s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00076s.
 Run 10:
  0.00126s: started 0
  40.36883s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00077s.
Completed maximum number of runs, terminated after 10 runs taking 393.24 seconds.
Writing data & meta-data took 0.01097s.
Overall took 393.28789s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03297s.

Running...
 Run 1:
  0.00115s: started 0
  40.32084s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00086s.
 Run 2:
  0.11899s: started 0
  39.27522s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
 Run 3:
  0.06285s: started 0
  39.18818s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00079s.
 Run 4:
  0.11365s: started 0
  39.2697s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00079s.
 Run 5:
  0.00129s: started 0
  40.03296s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00077s.
 Run 6:
  0.12903s: started 0
  39.15532s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00078s.
 Run 7:
  0.11278s: started 0
  39.1695s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00077s.
 Run 8:
  0.06233s: started 0
  39.00851s: awaited 0
  Data collection took 0.00014s.
  Convergence check took 0.00081s.
 Run 9:
  0.05278s: started 0
  38.75839s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00075s.
 Run 10:
  0.00125s: started 0
  39.5289s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00075s.
Completed maximum number of runs, terminated after 10 runs taking 393.72 seconds.
Writing data & meta-data took 0.01129s.
Overall took 393.76176s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.06969s.

Running...
 Run 1:
  0.00113s: started 0
  39.51348s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.51 seconds.
Writing data & meta-data took 0.00846s.
Overall took 39.59282s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04117s.

Running...
 Run 1:
  0.00112s: started 0
  39.08088s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00088s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.08 seconds.
Writing data & meta-data took 0.00826s.
Overall took 39.13152s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05635s.

Running...
 Run 1:
  0.00112s: started 0
  39.64492s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00088s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.65 seconds.
Writing data & meta-data took 0.00913s.
Overall took 39.71159s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04414s.

Running...
 Run 1:
  0.00109s: started 0
  39.58355s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.58 seconds.
Writing data & meta-data took 0.00855s.
Overall took 39.63743s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02459s.

Running...
 Run 1:
  0.00112s: started 0
  39.36624s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 39.37 seconds.
Writing data & meta-data took 0.00881s.
Overall took 39.40082s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05126s.

Running...
 Run 1:
  0.0011s: started 0
  40.38105s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 40.38 seconds.
Writing data & meta-data took 0.00916s.
Overall took 40.44262s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03694s.

Running...
 Run 1:
  0.00112s: started 0
  41.84057s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 41.84 seconds.
Writing data & meta-data took 0.00924s.
Overall took 41.88792s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-nccl/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-nccl/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04288s.

Running...
 Run 1:
  0.0011s: started 0
  55.35519s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00085s.
Completed after reaching confidence interval, terminated after 1 runs taking 55.36 seconds.
Writing data & meta-data took 0.00938s.
Overall took 55.40861s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0785s.

Running...
 Run 1:
  0.00108s: started 0
  31.23128s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.23 seconds.
Writing data & meta-data took 0.00925s.
Overall took 31.3202s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04627s.

Running...
 Run 1:
  0.00118s: started 0
  31.34717s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00085s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.35 seconds.
Writing data & meta-data took 0.00878s.
Overall took 31.40337s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05071s.

Running...
 Run 1:
  0.00111s: started 0
  31.0097s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00085s.
 Run 2:
  0.16303s: started 0
  30.4334s: awaited 0
  Data collection took 0.00012s.
  Convergence check took 0.00076s.
Completed after reaching confidence interval, terminated after 2 runs taking 61.45 seconds.
Writing data & meta-data took 0.00919s.
Overall took 61.50538s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03347s.

Running...
 Run 1:
  0.00109s: started 0
  30.60153s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 30.6 seconds.
Writing data & meta-data took 0.00934s.
Overall took 30.64552s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05048s.

Running...
 Run 1:
  0.00126s: started 0
  30.68549s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 30.69 seconds.
Writing data & meta-data took 0.00947s.
Overall took 30.74661s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04517s.

Running...
 Run 1:
  0.00108s: started 0
  30.72013s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00086s.
 Run 2:
  0.09195s: started 0
  30.22565s: awaited 0
  Data collection took 0.00013s.
  Convergence check took 0.00076s.
Completed after reaching confidence interval, terminated after 2 runs taking 60.95 seconds.
Writing data & meta-data took 0.00954s.
Overall took 61.00261s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04021s.

Running...
 Run 1:
  0.00109s: started 0
  31.10637s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.11 seconds.
Writing data & meta-data took 0.00909s.
Overall took 31.15682s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04831s.

Running...
 Run 1:
  0.00114s: started 0
  30.70592s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 30.71 seconds.
Writing data & meta-data took 0.00925s.
Overall took 30.76466s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05614s.

Running...
 Run 1:
  0.00112s: started 0
  31.37058s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.37 seconds.
Writing data & meta-data took 0.00881s.
Overall took 31.43672s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04451s.

Running...
 Run 1:
  0.0011s: started 0
  31.08375s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.08 seconds.
Writing data & meta-data took 0.00896s.
Overall took 31.13838s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05022s.

Running...
 Run 1:
  0.00118s: started 0
  31.42632s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00085s.
Completed after reaching confidence interval, terminated after 1 runs taking 31.43 seconds.
Writing data & meta-data took 0.00896s.
Overall took 31.48665s.
+ for schedule in ./apps_mix/gpubench/a2a-nccl/1B ./apps_mix/gpubench/a2a-nccl/4B ./apps_mix/gpubench/a2a-nccl/8B ./apps_mix/gpubench/a2a-nccl/64B ./apps_mix/gpubench/a2a-nccl/512B ./apps_mix/gpubench/a2a-nccl/4KiB ./apps_mix/gpubench/a2a-nccl/32KiB ./apps_mix/gpubench/a2a-nccl/256KiB ./apps_mix/gpubench/a2a-nccl/2MiB ./apps_mix/gpubench/a2a-nccl/16MiB ./apps_mix/gpubench/a2a-nccl/128MiB ./apps_mix/gpubench/a2a-nccl/1GiB ./apps_mix/gpubench/ar-nccl/1B ./apps_mix/gpubench/ar-nccl/4B ./apps_mix/gpubench/ar-nccl/8B ./apps_mix/gpubench/ar-nccl/64B ./apps_mix/gpubench/ar-nccl/512B ./apps_mix/gpubench/ar-nccl/4KiB ./apps_mix/gpubench/ar-nccl/32KiB ./apps_mix/gpubench/ar-nccl/256KiB ./apps_mix/gpubench/ar-nccl/2MiB ./apps_mix/gpubench/ar-nccl/16MiB ./apps_mix/gpubench/ar-nccl/128MiB ./apps_mix/gpubench/ar-nccl/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-nccl/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-nccl/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03942s.

Running...
 Run 1:
  0.00112s: started 0
  35.01476s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 35.02 seconds.
Writing data & meta-data took 0.00885s.
Overall took 35.06418s.
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05368s.

Running...
 Run 1:
  0.00112s: started 0
  4.52794s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.53 seconds.
Writing data & meta-data took 0.00802s.
Overall took 4.59104s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0489s.

Running...
 Run 1:
  0.00112s: started 0
  4.60194s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00086s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.6 seconds.
Writing data & meta-data took 0.00858s.
Overall took 4.66057s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03766s.

Running...
 Run 1:
  0.00116s: started 0
  4.38669s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.39 seconds.
Writing data & meta-data took 0.00889s.
Overall took 4.43432s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03716s.

Running...
 Run 1:
  0.00111s: started 0
  4.54009s: awaited 0
  Data collection took 0.0002s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.54 seconds.
Writing data & meta-data took 0.00846s.
Overall took 4.58678s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04058s.

Running...
 Run 1:
  0.00122s: started 0
  4.46988s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.47 seconds.
Writing data & meta-data took 0.00749s.
Overall took 4.51904s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03559s.

Running...
 Run 1:
  0.0011s: started 0
  4.61736s: awaited 0
  Data collection took 0.0002s.
  Convergence check took 0.00079s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.62 seconds.
Writing data & meta-data took 0.00821s.
Overall took 4.66223s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02764s.

Running...
 Run 1:
  0.0011s: started 0
  4.52233s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.52 seconds.
Writing data & meta-data took 0.00783s.
Overall took 4.55891s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0471s.

Running...
 Run 1:
  0.00124s: started 0
  4.50502s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.51 seconds.
Writing data & meta-data took 0.00831s.
Overall took 4.56154s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03301s.

Running...
 Run 1:
  0.00114s: started 0
  4.56591s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.57 seconds.
Writing data & meta-data took 0.00744s.
Overall took 4.60745s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03013s.

Running...
 Run 1:
  0.00112s: started 0
  4.66143s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00077s.
Completed after reaching confidence interval, terminated after 1 runs taking 4.66 seconds.
Writing data & meta-data took 0.00841s.
Overall took 4.70102s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0283s.

Running...
 Run 1:
  0.00116s: started 0
  5.287s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00083s.
Completed after reaching confidence interval, terminated after 1 runs taking 5.29 seconds.
Writing data & meta-data took 0.00764s.
Overall took 5.32407s.
+ for schedule in ./apps_mix/gpubench/pp-cudaaware/1B ./apps_mix/gpubench/pp-cudaaware/4B ./apps_mix/gpubench/pp-cudaaware/8B ./apps_mix/gpubench/pp-cudaaware/64B ./apps_mix/gpubench/pp-cudaaware/512B ./apps_mix/gpubench/pp-cudaaware/4KiB ./apps_mix/gpubench/pp-cudaaware/32KiB ./apps_mix/gpubench/pp-cudaaware/256KiB ./apps_mix/gpubench/pp-cudaaware/2MiB ./apps_mix/gpubench/pp-cudaaware/16MiB ./apps_mix/gpubench/pp-cudaaware/128MiB ./apps_mix/gpubench/pp-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/pp-cudaaware/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 2

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/pp-cudaaware/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=2, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 2
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03946s.

Running...
 Run 1:
  0.00112s: started 0
  6.43674s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.44 seconds.
Writing data & meta-data took 0.00873s.
Overall took 6.48604s.
[1;31m[error][0m a uenv is already running
[1;31m[error][0m a view is already loaded: {'path': PosixPath('/user-environment'), 'name': 'default'}
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.08001s.

Running...
 Run 1:
  0.00106s: started 0
  6.14928s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.15 seconds.
Writing data & meta-data took 0.00746s.
Overall took 6.23812s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.035s.

Running...
 Run 1:
  0.00112s: started 0
  6.56991s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.57 seconds.
Writing data & meta-data took 0.00792s.
Overall took 6.61393s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03117s.

Running...
 Run 1:
  0.00108s: started 0
  6.07597s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.08 seconds.
Writing data & meta-data took 0.0084s.
Overall took 6.11663s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02414s.

Running...
 Run 1:
  0.00114s: started 0
  6.29902s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.3 seconds.
Writing data & meta-data took 0.00927s.
Overall took 6.33355s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02946s.

Running...
 Run 1:
  0.00112s: started 0
  6.16791s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00085s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.17 seconds.
Writing data & meta-data took 0.00847s.
Overall took 6.20699s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04936s.

Running...
 Run 1:
  0.00108s: started 0
  6.38861s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00085s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.39 seconds.
Writing data & meta-data took 0.00757s.
Overall took 6.44671s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04526s.

Running...
 Run 1:
  0.00109s: started 0
  6.4738s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.47 seconds.
Writing data & meta-data took 0.00962s.
Overall took 6.52979s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03479s.

Running...
 Run 1:
  0.00112s: started 0
  6.40318s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00083s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.4 seconds.
Writing data & meta-data took 0.00837s.
Overall took 6.44747s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04184s.

Running...
 Run 1:
  0.00116s: started 0
  6.62719s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.63 seconds.
Writing data & meta-data took 0.00818s.
Overall took 6.67832s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03194s.

Running...
 Run 1:
  0.00107s: started 0
  6.56651s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.57 seconds.
Writing data & meta-data took 0.00815s.
Overall took 6.60771s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04135s.

Running...
 Run 1:
  0.00109s: started 0
  8.45844s: awaited 0
  Data collection took 0.00024s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 8.46 seconds.
Writing data & meta-data took 0.00859s.
Overall took 8.50953s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/a2a-cudaaware/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/a2a-cudaaware/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03846s.

Running...
 Run 1:
  0.00112s: started 0
  18.83721s: awaited 0
  18.83723s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 1] [job id 45848.166] [Fri Mar 22 12:01:21 2024] [nid005128] - Abort(277422082) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
MPICH ERROR [Rank 0] [job id 45848.166] [Fri Mar 22 12:01:21 2024] [nid005128] - Abort(277422082) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 1: Exited with exit code 255
srun: Terminating StepId=45848.166
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.166 ON nid005128 CANCELLED AT 2024-03-22T12:01:22 ***
srun: error: nid005128: task 0: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 2:
  0.00127s: started 0
  18.02564s: awaited 0
  18.02567s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 1] [job id 45848.167] [Fri Mar 22 12:01:39 2024] [nid005128] - Abort(344530946) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 0] [job id 45848.167] [Fri Mar 22 12:01:39 2024] [nid005128] - Abort(344530946) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.167
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.167 ON nid005128 CANCELLED AT 2024-03-22T12:01:40 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 3:
  0.00124s: started 0
  17.74544s: awaited 0
  17.74546s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 0] [job id 45848.168] [Fri Mar 22 12:01:57 2024] [nid005128] - Abort(277422082) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
MPICH ERROR [Rank 1] [job id 45848.168] [Fri Mar 22 12:01:57 2024] [nid005128] - Abort(344530946) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.168
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.168 ON nid005128 CANCELLED AT 2024-03-22T12:01:57 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 4:
  0.00127s: started 0
  17.26847s: awaited 0
  17.26849s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 0] [job id 45848.169] [Fri Mar 22 12:02:14 2024] [nid005128] - Abort(277422082) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 1] [job id 45848.169] [Fri Mar 22 12:02:14 2024] [nid005128] - Abort(344530946) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.169
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.169 ON nid005128 CANCELLED AT 2024-03-22T12:02:15 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 5:
  0.00127s: started 0
  18.3346s: awaited 0
  18.33462s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 0] [job id 45848.170] [Fri Mar 22 12:02:33 2024] [nid005128] - Abort(277422082) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 1] [job id 45848.170] [Fri Mar 22 12:02:33 2024] [nid005128] - Abort(545857538) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff920000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa20000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff920000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa20000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.170
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.170 ON nid005128 CANCELLED AT 2024-03-22T12:02:33 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 6:
  0.00125s: started 0
  17.4922s: awaited 0
  17.49223s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 0] [job id 45848.171] [Fri Mar 22 12:02:50 2024] [nid005128] - Abort(277422082) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
MPICH ERROR [Rank 1] [job id 45848.171] [Fri Mar 22 12:02:50 2024] [nid005128] - Abort(277422082) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 1: Exited with exit code 255
srun: Terminating StepId=45848.171
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.171 ON nid005128 CANCELLED AT 2024-03-22T12:02:51 ***
srun: error: nid005128: task 0: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 7:
  0.0012s: started 0
  17.81268s: awaited 0
  17.8127s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 1] [job id 45848.172] [Fri Mar 22 12:03:08 2024] [nid005128] - Abort(277422082) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 0] [job id 45848.172] [Fri Mar 22 12:03:08 2024] [nid005128] - Abort(545857538) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff920000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa20000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff920000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa20000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.172
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.172 ON nid005128 CANCELLED AT 2024-03-22T12:03:08 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 8:
  0.00126s: started 0
  18.18643s: awaited 0
  18.18646s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 1] [job id 45848.173] [Fri Mar 22 12:03:26 2024] [nid005128] - Abort(277422082) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 0] [job id 45848.173] [Fri Mar 22 12:03:26 2024] [nid005128] - Abort(344530946) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: tasks 0-1: Exited with exit code 255
srun: Terminating StepId=45848.173
srun: error: nid005128: task 3: Segmentation fault
slurmstepd: error: *** STEP 45848.173 ON nid005128 CANCELLED AT 2024-03-22T12:03:26 ***
srun: error: nid005128: task 2: Segmentation fault

  Data collection took 2e-05s.
  Convergence check took 0.0s.
 Run 9:
  0.00119s: started 0
  18.38624s: awaited 0
  18.38626s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 1] [job id 45848.174] [Fri Mar 22 12:03:45 2024] [nid005128] - Abort(344530946) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

MPICH ERROR [Rank 0] [job id 45848.174] [Fri Mar 22 12:03:45 2024] [nid005128] - Abort(344530946) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 1: Exited with exit code 255
srun: Terminating StepId=45848.174
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.174 ON nid005128 CANCELLED AT 2024-03-22T12:03:45 ***
srun: error: nid005128: task 0: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
 Run 10:
  0.00121s: started 0
  17.8401s: awaited 0
  17.84012s: encountered an exception in 0
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 0) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
(GTL DEBUG: 1) cuIpcGetMemHandle: invalid argument, CUDA_ERROR_INVALID_VALUE, line no 148
MPICH ERROR [Rank 0] [job id 45848.175] [Fri Mar 22 12:04:02 2024] [nid005128] - Abort(344530946) (rank 0 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff8e0000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfff9e0000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
MPICH ERROR [Rank 1] [job id 45848.175] [Fri Mar 22 12:04:02 2024] [nid005128] - Abort(277422082) (rank 1 in comm 0): Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count

aborting job:
Fatal error in PMPI_Alltoall: Invalid count, error stack:
PMPI_Alltoall(427)...................: MPI_Alltoall(sbuf=0xfff900000000, scount=1073741824, dtype=0x4c00013b, rbuf=0xfffa00000000, rcount=1073741824, datatype=dtype=0x4c00013b, comm=MPI_COMM_WORLD) failed
MPIR_Alltoall_impl(259)..............: 
MPIR_Alltoall_intra_auto(170)........: Failure during collective
MPIR_Alltoall_intra_auto(166)........: 
MPIR_Alltoall_intra_pairwise(95).....: 
MPIC_Sendrecv(334)...................: 
MPID_Isend_coll(610).................: 
MPIDI_isend_coll_unsafe(176).........: 
MPIDI_SHM_mpi_isend(323).............: 
MPIDI_CRAY_Common_lmt_isend(84)......: 
MPIDI_CRAY_Common_lmt_export_mem(103): 
(unknown)(): Invalid count
srun: error: nid005128: task 0: Exited with exit code 255
srun: Terminating StepId=45848.175
srun: error: nid005128: tasks 2-3: Segmentation fault
slurmstepd: error: *** STEP 45848.175 ON nid005128 CANCELLED AT 2024-03-22T12:04:03 ***
srun: error: nid005128: task 1: Exited with exit code 255

  Data collection took 1e-05s.
  Convergence check took 0.0s.
Completed maximum number of runs, terminated after 10 runs taking 179.93 seconds.
Traceback (most recent call last):
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 595, in <module>
    main()
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 580, in main
    log_data(out_format, data_directory+'/data', data_container_list)
  File "/bret/scratch/cscs/lfusco/blink/runner.py", line 184, in log_data
    dataframe.columns = columns
    ^^^^^^^^^^^^^^^^^
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/generic.py", line 6310, in __setattr__
    return object.__setattr__(self, name, value)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "properties.pyx", line 69, in pandas._libs.properties.AxisProperty.__set__
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/generic.py", line 813, in _set_axis
    self._mgr.set_axis(axis, labels)
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/internals/managers.py", line 238, in set_axis
    self._validate_set_axis(axis, new_labels)
  File "/users/lfusco/.local/lib/python3.11/site-packages/pandas/core/internals/base.py", line 98, in _validate_set_axis
    raise ValueError(
ValueError: Length mismatch: Expected axis has 0 elements, new values have 2 elements
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/1B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/1B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 0", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.08722s.

Running...
 Run 1:
  0.00111s: started 0
  6.24413s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.25 seconds.
Writing data & meta-data took 0.00841s.
Overall took 6.34109s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/4B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/4B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 2", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04728s.

Running...
 Run 1:
  0.0011s: started 0
  6.13212s: awaited 0
  Data collection took 0.00021s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.13 seconds.
Writing data & meta-data took 0.00844s.
Overall took 6.18893s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/8B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/8B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 3", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04869s.

Running...
 Run 1:
  0.00111s: started 0
  5.58223s: awaited 0
  Data collection took 0.00025s.
  Convergence check took 0.00092s.
Completed after reaching confidence interval, terminated after 1 runs taking 5.58 seconds.
Writing data & meta-data took 0.00875s.
Overall took 5.64091s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/64B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/64B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 6", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03327s.

Running...
 Run 1:
  0.00114s: started 0
  5.85312s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 5.85 seconds.
Writing data & meta-data took 0.00793s.
Overall took 5.89542s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/512B auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/512B', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 9", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.02723s.

Running...
 Run 1:
  0.00122s: started 0
  5.86199s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 5.86 seconds.
Writing data & meta-data took 0.00897s.
Overall took 5.89931s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/4KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/4KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 12", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04382s.

Running...
 Run 1:
  0.00111s: started 0
  6.40922s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.41 seconds.
Writing data & meta-data took 0.00819s.
Overall took 6.46235s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/32KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/32KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 15", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04123s.

Running...
 Run 1:
  0.00111s: started 0
  6.30003s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00083s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.3 seconds.
Writing data & meta-data took 0.0083s.
Overall took 6.35067s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/256KiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/256KiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 18", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04531s.

Running...
 Run 1:
  0.00108s: started 0
  6.57934s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.0008s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.58 seconds.
Writing data & meta-data took 0.00807s.
Overall took 6.63382s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/2MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/2MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 21", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.05161s.

Running...
 Run 1:
  0.00112s: started 0
  6.51298s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.51 seconds.
Writing data & meta-data took 0.00789s.
Overall took 6.57359s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/16MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/16MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 24", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.04852s.

Running...
 Run 1:
  0.00111s: started 0
  6.84685s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00081s.
Completed after reaching confidence interval, terminated after 1 runs taking 6.85 seconds.
Writing data & meta-data took 0.00773s.
Overall took 6.90423s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/128MiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/128MiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 27", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.03871s.

Running...
 Run 1:
  0.00111s: started 0
  7.63944s: awaited 0
  Data collection took 0.00023s.
  Convergence check took 0.00084s.
Completed after reaching confidence interval, terminated after 1 runs taking 7.64 seconds.
Writing data & meta-data took 0.0095s.
Overall took 7.68879s.
+ for schedule in ./apps_mix/gpubench/a2a-cudaaware/1B ./apps_mix/gpubench/a2a-cudaaware/4B ./apps_mix/gpubench/a2a-cudaaware/8B ./apps_mix/gpubench/a2a-cudaaware/64B ./apps_mix/gpubench/a2a-cudaaware/512B ./apps_mix/gpubench/a2a-cudaaware/4KiB ./apps_mix/gpubench/a2a-cudaaware/32KiB ./apps_mix/gpubench/a2a-cudaaware/256KiB ./apps_mix/gpubench/a2a-cudaaware/2MiB ./apps_mix/gpubench/a2a-cudaaware/16MiB ./apps_mix/gpubench/a2a-cudaaware/128MiB ./apps_mix/gpubench/a2a-cudaaware/1GiB ./apps_mix/gpubench/ar-cudaaware/1B ./apps_mix/gpubench/ar-cudaaware/4B ./apps_mix/gpubench/ar-cudaaware/8B ./apps_mix/gpubench/ar-cudaaware/64B ./apps_mix/gpubench/ar-cudaaware/512B ./apps_mix/gpubench/ar-cudaaware/4KiB ./apps_mix/gpubench/ar-cudaaware/32KiB ./apps_mix/gpubench/ar-cudaaware/256KiB ./apps_mix/gpubench/ar-cudaaware/2MiB ./apps_mix/gpubench/ar-cudaaware/16MiB ./apps_mix/gpubench/ar-cudaaware/128MiB ./apps_mix/gpubench/ar-cudaaware/1GiB
+ for nam in l
+ for split in 100
+ python3 runner.py ./apps_mix/gpubench/ar-cudaaware/1GiB auto -am l -as 100 -n 1 -mn 1 -mx 10 -t 600 -a 0.05 -b 0.05 -of csv -ro +file -p 4

Passed arguments:
Namespace(app_mix='./apps_mix/gpubench/ar-cudaaware/1GiB', node_file='auto', numnodes=1, allocationmode='l', allocationsplit='100', minruns=1, maxruns=10, timeout=600.0, alpha=0.05, beta=0.05, ppn=4, convergeall=False, outformat='csv', runtimeout='+file', seed=1, datapath='./data', extrainfo=None, replace_mix_args=None)

Apps:
0: with arguments:"-l 100 -x 30", collection flag: True, ending condition: run until finished.
Split list: [100.0]
Splits: [1]

Node allocation:
Processes per node (ppn): 4
0 on 1 nodes:
['nid005128']

Schedule:
0.0s: start 0

Preparing took 0.0369s.

Running...
 Run 1:
  0.00111s: started 0
  11.17987s: awaited 0
  Data collection took 0.00022s.
  Convergence check took 0.00082s.
Completed after reaching confidence interval, terminated after 1 runs taking 11.18 seconds.
Writing data & meta-data took 0.00939s.
Overall took 11.22728s.
